{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"cf_etl\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL and Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_USERNAME = os.getenv('DB_USERNAME')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "\n",
    "def table_df(schema_name, table_name):\n",
    "    url = f\"jdbc:mysql://{DB_HOST}:{DB_PORT}/{schema_name}\"\n",
    "    properties = {\n",
    "        \"user\": DB_USERNAME,\n",
    "        \"password\": DB_PASSWORD,\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "    df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table_df('config_db','cf_etl_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Config Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_date_on_config_table(schema_name, table_name, index):\n",
    "\n",
    "    pymysql_connection = pymysql.connect(\n",
    "        host= DB_HOST,\n",
    "        user= DB_USERNAME,\n",
    "        password=DB_PASSWORD,\n",
    "        database= schema_name\n",
    "    )\n",
    "\n",
    "    with pymysql_connection.cursor() as cursor:\n",
    "        exec_date_query = f\"update `{schema_name}`.{table_name} set execution_date = (current_timestamp) where id = {index+1}\"\n",
    "        cursor.execute(exec_date_query)\n",
    "\n",
    "        update_startdate_query = f\"update `{schema_name}`.{table_name}  set start_date_time = date_add(start_date_time, interval 1 day)\"\n",
    "        cursor.execute(update_startdate_query)\n",
    "\n",
    "        update_enddate_query = f\"update `{schema_name}`.{table_name}  set end_date_time = date_add(end_date_time, interval 1 day)\"\n",
    "        cursor.execute(update_enddate_query)\n",
    "        \n",
    "        pymysql_connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading File to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload():\n",
    "\n",
    "    df = table_df('config_db','cf_etl_table')\n",
    "\n",
    "    for i, row in zip(range(df.count()), df.collect()):\n",
    "        is_incremental, schema, table, location, hdfs_file = row['is_incremental'], row['schema_name'], row['table_name'], row['hdfs_upload_location'], row['hdfs_file_name'] \n",
    "        hdfs_path = f\"{location}{hdfs_file}\"\n",
    "        \n",
    "        if is_incremental:\n",
    "            start_date, end_date, date_col = row['start_date_time'], row['end_date_time'], row['inc_field']\n",
    " \n",
    "            query = f\"(SELECT * FROM {schema}.{table} WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}') AS sql_query\"\n",
    "            result = table_df(schema, query)\n",
    "            result.write.mode('append').parquet(hdfs_path)\n",
    "            \n",
    "            update_date_on_config_table('config_db', 'cf_etl_table', i)\n",
    "\n",
    "        elif not is_incremental:\n",
    "            result = table_df(schema, table)\n",
    "            result.write.mode(\"overwrite\").parquet(hdfs_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_column_value(df, index, col):\n",
    "#     \"\"\"\n",
    "#     This function retrieves a value from a specified column in a DataFrame at a given index.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     col_values = df.select(col).collect()\n",
    "#     value = col_values[index][col]\n",
    "#     return value\n",
    "\n",
    "# def upload():\n",
    "\n",
    "#     df = table_df('config_db','cf_etl_table')\n",
    "\n",
    "#     for i in range(df.count()):\n",
    "        \n",
    "#         is_incremental = get_column_value(df, i, 'is_incremental')\n",
    "#         schema = get_column_value(df, i, 'schema_name')\n",
    "#         table = get_column_value(df, i, 'table_name')\n",
    "#         location = get_column_value(df, i, 'hdfs_upload_location')\n",
    "#         hdfs_file = get_column_value(df, i, 'hdfs_file_name')\n",
    "#         hdfs_path = f\"{location}{hdfs_file}\"\n",
    "\n",
    "#         if is_incremental:\n",
    "#             start_date = get_column_value(df, i, 'start_date_time')\n",
    "#             end_date = get_column_value(df, i, 'end_date_time')\n",
    "#             date_col = get_column_value(df, i, 'inc_field')      \n",
    " \n",
    "#             query = f\"(SELECT * FROM {schema}.{table} WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}') AS sql_query\"\n",
    "#             result = table_df(schema, query)\n",
    "#             result.write.mode('append').parquet(hdfs_path)\n",
    "            \n",
    "#             update_date_on_config_table('config_db', 'cf_etl_table', i)\n",
    "\n",
    "#         elif not is_incremental:\n",
    "#             result = table_df(schema, table)\n",
    "#             result.write.mode(\"overwrite\").parquet(hdfs_path)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.read.parquet('hdfs://localhost:19000//mydir/transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+--------+------+\n",
      "|tnx_id|           tnx_date|acc_id| product|status|\n",
      "+------+-------------------+------+--------+------+\n",
      "|     5|2023-06-27 14:30:00|ACC005|ProductE| false|\n",
      "|     2|2023-06-24 11:00:00|ACC002|ProductB| false|\n",
      "|     1|2023-06-23 10:30:00|ACC001|ProductA| false|\n",
      "|     4|2023-06-26 12:15:00|ACC004|ProductD| false|\n",
      "|     1|2023-06-23 10:30:00|ACC001|ProductA| false|\n",
      "|     1|2023-06-23 10:30:00|ACC001|ProductA| false|\n",
      "|     1|2023-06-23 10:30:00|ACC001|ProductA| false|\n",
      "|     3|2023-06-25 09:45:00|ACC003|ProductC| false|\n",
      "+------+-------------------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if Hadoop File Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Incremental Load') \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:19000\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "\n",
    "hdfs_path = \"hdfs://localhost:19000//mydir/transaction\"\n",
    "\n",
    "path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)\n",
    "if fs.exists(path):\n",
    "    print('file exists')\n",
    "else:\n",
    "    print('file doesn\\'t exist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
