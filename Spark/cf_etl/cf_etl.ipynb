{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pymysql\n",
    "\n",
    "spark = SparkSession.builder.appName(\"cf_etl\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL and Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_USERNAME = os.getenv('DB_USERNAME')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "\n",
    "def table_df(schema_name, table_name):\n",
    "    url = f\"jdbc:mysql://{DB_HOST}:{DB_PORT}/{schema_name}\"\n",
    "    properties = {\n",
    "        \"user\": DB_USERNAME,\n",
    "        \"password\": DB_PASSWORD,\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "    df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Config Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_date_on_config_table(schema_name, table_name, index, interval_period):\n",
    "\n",
    "    pymysql_connection = pymysql.connect(\n",
    "        host= DB_HOST,\n",
    "        user= DB_USERNAME,\n",
    "        password=DB_PASSWORD,\n",
    "        database= schema_name\n",
    "    )\n",
    "\n",
    "    with pymysql_connection.cursor() as cursor:\n",
    "        exec_date_query = f\"update `{schema_name}`.{table_name} set execution_date = (current_timestamp) where table_id = {index+1}\"\n",
    "        cursor.execute(exec_date_query)\n",
    "\n",
    "        update_startdate_query = f\"update `{schema_name}`.{table_name}  set start_date_time = date_add(start_date_time, interval {interval_period} day)\"\n",
    "        cursor.execute(update_startdate_query)\n",
    "\n",
    "        update_enddate_query = f\"update `{schema_name}`.{table_name}  set end_date_time = date_add(end_date_time, interval {interval_period} day)\"\n",
    "        cursor.execute(update_enddate_query)\n",
    "        \n",
    "        pymysql_connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table_df('config_db','cf_etl_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------------+--------------------+-------------------+--------------+----------------+-------------------+-------------------+--------------+---------------+---------------+\n",
      "|table_id|   schema_name|         table_name|hdfs_upload_location|     hdfs_file_name|is_incremental|       inc_field|    start_date_time|      end_date_time|execution_date|interval_period|   partition_by|\n",
      "+--------+--------------+-------------------+--------------------+-------------------+--------------+----------------+-------------------+-------------------+--------------+---------------+---------------+\n",
      "|       1|transaction_db|        transaction|hdfs://localhost:...|        transaction|          true|transaction_date|2023-06-25 00:00:00|2023-06-25 23:59:59|    2024-06-25|              1|trans_date_only|\n",
      "|       2|transaction_db|transaction_non_inc|hdfs://localhost:...|transaction_non_inc|         false|            NULL|               NULL|               NULL|          NULL|           NULL|           NULL|\n",
      "+--------+--------------+-------------------+--------------------+-------------------+--------------+----------------+-------------------+-------------------+--------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive Field Mapped Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_mapped_df(cf_db, schema_name, table_name, table_id):\n",
    "\n",
    "    con = pymysql.connect(\n",
    "        host= DB_HOST,\n",
    "        user= DB_USERNAME,\n",
    "        password=DB_PASSWORD,\n",
    "        database= cf_db\n",
    "    )\n",
    "\n",
    "    with con.cursor() as cursor:\n",
    "        cursor.callproc(f'{cf_db}.sp_field_mapping', [schema_name, table_name, table_id])\n",
    "        result = cursor.fetchall()\n",
    "        fields = [desc[0] for desc in cursor.description]   \n",
    "        df = spark.createDataFrame(result, fields)    \n",
    "        con.commit()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading File to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload():\n",
    "\n",
    "    df = table_df('config_db','cf_etl_table')\n",
    "\n",
    "    for i, row in zip(range(df.count()), df.collect()):\n",
    "        is_incremental, table_id, schema, table, location, hdfs_file = row['is_incremental'], row['table_id'], row['schema_name'], row['table_name'], row['hdfs_upload_location'], row['hdfs_file_name'] \n",
    "        hdfs_path = f\"{location}{hdfs_file}\"\n",
    "\n",
    "        field_mapped_table = field_mapped_df('config_db', schema, table, table_id)\n",
    "        \n",
    "        if is_incremental:\n",
    "            start_date, end_date, date_col, interval_period, partition_by = row['start_date_time'], row['end_date_time'], row['inc_field'], row['interval_period'], row['partition_by']\n",
    "\n",
    "            field_mapped_table.createOrReplaceTempView(\"incremental_table\")\n",
    "\n",
    "            result = spark.sql(f\"SELECT * FROM incremental_table WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'\")\n",
    "            result.write.mode('append').parquet(hdfs_path, partitionBy = partition_by)\n",
    "            \n",
    "            update_date_on_config_table('config_db', 'cf_etl_table', i, interval_period)\n",
    "\n",
    "        elif not is_incremental:\n",
    "            field_mapped_table.write.mode(\"overwrite\").parquet(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_column_value(df, index, col):\n",
    "#     \"\"\"\n",
    "#     This function retrieves a value from a specified column in a DataFrame at a given index.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     col_values = df.select(col).collect()\n",
    "#     value = col_values[index][col]\n",
    "#     return value\n",
    "\n",
    "# def upload():\n",
    "\n",
    "#     df = table_df('config_db','cf_etl_table')\n",
    "\n",
    "#     for i in range(df.count()):\n",
    "        \n",
    "#         is_incremental = get_column_value(df, i, 'is_incremental')\n",
    "#         schema = get_column_value(df, i, 'schema_name')\n",
    "#         table = get_column_value(df, i, 'table_name')\n",
    "#         location = get_column_value(df, i, 'hdfs_upload_location')\n",
    "#         hdfs_file = get_column_value(df, i, 'hdfs_file_name')\n",
    "#         hdfs_path = f\"{location}{hdfs_file}\"\n",
    "\n",
    "#         if is_incremental:\n",
    "#             start_date = get_column_value(df, i, 'start_date_time')\n",
    "#             end_date = get_column_value(df, i, 'end_date_time')\n",
    "#             date_col = get_column_value(df, i, 'inc_field')      \n",
    " \n",
    "#             query = f\"(SELECT * FROM {schema}.{table} WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}') AS sql_query\"\n",
    "#             result = table_df(schema, query)\n",
    "#             result.write.mode('append').parquet(hdfs_path)\n",
    "            \n",
    "#             update_date_on_config_table('config_db', 'cf_etl_table', i)\n",
    "\n",
    "#         elif not is_incremental:\n",
    "#             result = table_df(schema, table)\n",
    "#             result.write.mode(\"overwrite\").parquet(hdfs_path)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.read.parquet('hdfs://localhost:19000//mydir/transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+------------+---------------+\n",
      "|transaction_id|   transaction_date|account_id|product_name|trans_date_only|\n",
      "+--------------+-------------------+----------+------------+---------------+\n",
      "|             1|2023-06-23 10:30:00|    ACC001|    ProductA|     2023-06-23|\n",
      "+--------------+-------------------+----------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if Hadoop File Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Incremental Load') \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:19000\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "\n",
    "hdfs_path = \"hdfs://localhost:19000//mydir/transaction\"\n",
    "\n",
    "path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)\n",
    "if fs.exists(path):\n",
    "    print('file exists')\n",
    "else:\n",
    "    print('file doesn\\'t exist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
